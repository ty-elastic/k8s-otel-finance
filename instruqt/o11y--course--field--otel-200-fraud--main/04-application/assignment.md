---
slug: application
id: ylynqfneshqn
type: challenge
title: Applying Our Model
tabs:
- id: xs7t9jhyoxnu
  title: Elastic
  type: service
  hostname: kubernetes-vm
  path: /app/ml/trained_models
  port: 30001
difficulty: ""
timelimit: 0
enhanced_loading: null
---
Now that we've built a model, we can deploy and apply it against simulated trades being generated by our `monkey` service to see if it can identify and label trades as possibly fraudulent.

To apply the model we just trained, we will be creating an Ingest Pipeline. That Ingest Pipeline could then be applied in real-time to new trades transactions, or can be applied to batches of transaction data. Given the CPU overhead associated with running a model, typically you would run the model in a batch, non-realtime manner.

# Create an Ingest Pipeline
1. On our Trained Models page, find the model `classification-*`
2. On the right hand side of row containing our model, click `...` to open the contextual menu and select `Deploy model`
  ![View](../assets/application_deploy.png)
3. Under `1. Details` select `Continue`
4. Under `2. Configure processor` select `Continue`
5. Under `3. Handle failures` select `Continue`
6. Under `4. Test (Optional)` select `Continue`
7. Under `5. Create` select `Create pipeline`

## Next steps
Now we will apply that ingest pipeline to all of the trade transactions to date.

Open `Next steps`:
1. Open `Reindex with pipeline`
2. Set `Destination index name` to
  ```
  classified_trades
  ```
3. Enable `Create data view`
  ![View](../assets/application_reindex.png)
4. Click `Reindex`
5. Click 'Close'

Checking our accuracy
===
Once the re-indexing (batch) operation is complete, we can have a look at the results.

# Discover
Let's first spot check things in Discover to ensure at least some transactions were labeled as fraudulent.
1. In Elastic, use the navigation pane to navigate to `Discover` and then select the `Discover` tab (and not `Logs Explorer`)
2. Click `Try ES|QL`
3. We want to look specifically at _non-training_ data which was labeled as fraudulent. To do that, enter the following ES|QL
  ```
  FROM classified_trades |
  WHERE attributes.com.example.data_source == "monkey" |
  LIMIT 100 |
  KEEP attributes.com.example.trade_id, attributes.com.example.action, attributes.com.example.day_of_week, attributes.com.example.region, attributes.com.example.share_price, attributes.com.example.shares, attributes.com.example.symbol, ml.inference.classification.classification, ml.inference.classification.prediction_probability
  ```

Note `ml.inference.classification.classification`: that's the field Elasticsearch injected into every record after running trades against the model we built. Have a look at the trades which the model predicted as fraudulent; they should fall within the pattern of trades you generated as a criminal when we trained our model (refer to that screen snapshot you took).

# Custom Dashboard
Now that's we've confirmed we are labeling transactions as fraudulent, let's visualize and validate our results!

We've created a custom dashboard which we can use to visually compare the classification of transactions against the features of the transactions.

1. In Elastic, navigate to `Dashboards` > `Fraudulent Transactions`
2. Validate that graphs match the pattern of fraudulent transactions you generated (refer to that screen snapshot you took)

> [!NOTE]
> This dashboard is filtering out training data. You are looking solely at the transactions made by our `monkey` service.

For example, if, as a criminal, you traded only on Mondays and Wednesdays, you should see transactions that `monkey` made on Mondays and Wednesdays as labeled partially fraudulent. Why partially? Because our pattern wasn't _simply_ Monday and Wednesdays; there are other many other variables in play. Conversely, you wouldn't see transactions on Tuesday and Thursday labeled fraudulent.

Summary
===
In less than an hour we were able to build, deploy, and validate a model which can successful predict fraudulent transactions. All within Elasticsearch and without requiring months of work by your data science team!

We created this workshop to show just how easy it is with Elastic to get additional value out of your existing APM data. You could imagine applying this concept to many different industries. Furthermore, `classification` isn't limited to being a single value (e.g., `fraud`); you can Elastic to model and predict many different values (e.g., labeling certain transactions one way, and others another).

Want to learn more about Elastic's support for [OpenTelemetry](https://www.elastic.co/what-is/opentelemetry) or our out-of-the-box [Machine Learning](https://www.elastic.co/elasticsearch/machine-learning)? [Reach out to us!](https://www.elastic.co/contact)