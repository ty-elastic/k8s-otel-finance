# For installation and configuration options, refer to the [installation instructions](https://github.com/elastic/opentelemetry/blob/main/docs/kubernetes/operator/README.md)

# For advanced configuration options, refer to the [official OpenTelemetry Helm chart](https://github.com/open-telemetry/opentelemetry-helm-charts/blob/main/charts/opentelemetry-kube-stack/values.yaml)
# This file has been tested together with opentelemetry-kube-stack helm chart version: 0.3.3
opentelemetry-operator:
  manager:
    extraArgs:
      - --enable-go-instrumentation
      - --enable-nginx-instrumentation
  admissionWebhooks:
    certManager:
      enabled: false # For production environments, it is [recommended to use cert-manager for better security and scalability](https://github.com/open-telemetry/opentelemetry-helm-charts/tree/main/charts/opentelemetry-operator#tls-certificate-requirement).
    autoGenerateCert:
      enabled: true # Enable/disable automatic certificate generation. Set to false if manually managing certificates.
      recreate: true # Force certificate regeneration on updates. Only applicable if autoGenerateCert.enabled is true.
crds:
  create: true # Install the OpenTelemetry Operator CRDs.
defaultCRConfig:
  image:
    repository: "us-central1-docker.pkg.dev/elastic-sa/tbekiares/otelcol"
    tag: "latest"
  imagePullPolicy: Always
  targetAllocator:
    enabled: false # Enable/disable the Operator's Target allocator.
    # Refer to: https://github.com/open-telemetry/opentelemetry-operator/tree/main/cmd/otel-allocator
clusterRole:
  rules:
    - apiGroups: [""]
      resources: ["configmaps"]
      verbs: ["get"]
# `clusterName` specifies the name of the Kubernetes cluster. It sets the 'k8s.cluster.name' field.
# Cluster Name is automatically detected for EKS/GKE/AKS. Add the below value in environments where cluster name cannot be detected.
# clusterName: myClusterName
collectors:
  #  Cluster is a K8s deployment EDOT collector focused on gathering telemetry
  #  at the cluster level (Kubernetes Events and cluster metrics).
  cluster:
    fullnameOverride: "opentelemetry-kube-stack-cluster-stats"
    env:
      - name: ELASTIC_AGENT_OTEL
        value: '"true"'
    config:
      exporters:
        # [Debug exporter](https://github.com/open-telemetry/opentelemetry-collector/blob/main/exporter/debugexporter/README.md)
        debug:
          verbosity: basic # Options: basic, detailed. Choose verbosity level for debug logs.
          # [Elasticsearch exporter](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/elasticsearchexporter/README.md)
        otlp/gateway:
          endpoint: "http://opentelemetry-kube-stack-gateway-collector:4317"
          tls:
            insecure: true
      processors:
        # [Resource Detection Processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourcedetectionprocessor)
        resourcedetection/eks:
          detectors: [env, eks] # Detects resources from environment variables and EKS (Elastic Kubernetes Service).
          timeout: 15s
          override: true
          eks:
            resource_attributes:
              k8s.cluster.name:
                enabled: true
        resourcedetection/gcp:
          detectors: [env, gcp] # Detects resources from environment variables and GCP (Google Cloud Platform).
          timeout: 2s
          override: true
        resourcedetection/aks:
          detectors: [env, aks] # Detects resources from environment variables and AKS (Azure Kubernetes Service).
          timeout: 2s
          override: true
          aks:
            resource_attributes:
              k8s.cluster.name:
                enabled: true
        # [Resource Processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourceprocessor)
        resource/k8s: # Resource attributes tailored for services within Kubernetes.
          attributes:
            - key: service.name # Set the service.name resource attribute based on the well-known app.kubernetes.io/name label
              from_attribute: app.label.name
              action: insert
            - key: service.name # Set the service.name resource attribute based on the k8s.container.name attribute
              from_attribute: k8s.container.name
              action: insert
            - key: app.label.name # Delete app.label.name attribute previously used for service.name
              action: delete
            - key: service.version # Set the service.version resource attribute based on the well-known app.kubernetes.io/version label
              from_attribute: app.label.version
              action: insert
            - key: app.label.version # Delete app.label.version attribute previously used for service.version
              action: delete
        resource/hostname:
          attributes:
            - key: host.name
              from_attribute: k8s.node.name
              action: upsert
        # [K8s Attributes Processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/k8sattributesprocessor)
        k8sattributes:
          passthrough: false # Annotates resources with the pod IP and does not try to extract any other metadata.
          pod_association:
            # Below association takes a look at the k8s.pod.ip and k8s.pod.uid resource attributes or connection's context, and tries to match it with the pod having the same attribute.
            - sources:
                - from: resource_attribute
                  name: k8s.pod.ip
            - sources:
                - from: resource_attribute
                  name: k8s.pod.uid
            - sources:
                - from: connection
          extract:
            metadata:
              - "k8s.namespace.name"
              - "k8s.deployment.name"
              - "k8s.replicaset.name"
              - "k8s.statefulset.name"
              - "k8s.daemonset.name"
              - "k8s.cronjob.name"
              - "k8s.job.name"
              - "k8s.node.name"
              - "k8s.pod.name"
              - "k8s.pod.ip"
              - "k8s.pod.uid"
              - "k8s.pod.start_time"
            labels:
              - tag_name: app.label.name
                key: app.kubernetes.io/name
                from: pod
              - tag_name: app.label.version
                key: app.kubernetes.io/version
                from: pod
      receivers:
        # [K8s Objects Receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/k8sobjectsreceiver)
        k8sobjects:
          objects:
            - name: events
              mode: "watch"
              group: "events.k8s.io"
              exclude_watch_type:
                - "DELETED"
        # [K8s Cluster Receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/k8sclusterreceiver)
        k8s_cluster:
          auth_type: serviceAccount # Determines how to authenticate to the K8s API server. This can be one of none (for no auth), serviceAccount (to use the standard service account token provided to the agent pod), or kubeConfig to use credentials from ~/.kube/config.
          node_conditions_to_report:
            - Ready
            - MemoryPressure
          allocatable_types_to_report:
            - cpu
            - memory
          metrics:
            k8s.pod.status_reason:
              enabled: true
          resource_attributes:
            k8s.kubelet.version:
              enabled: true
            os.description:
              enabled: true
            os.type:
              enabled: true
            k8s.container.status.last_terminated_reason:
              enabled: true
      # [Service Section](https://opentelemetry.io/docs/collector/configuration/#service)
      service:
        pipelines:
          metrics:
            exporters:
              - debug
              - otlp/gateway
            processors:
              - k8sattributes
              - resourcedetection/eks
              - resourcedetection/gcp
              - resourcedetection/aks
              - resource/k8s
              - resource/hostname
            receivers:
              - k8s_cluster
          logs:
            receivers:
              - k8sobjects
            processors:
              - resourcedetection/eks
              - resourcedetection/gcp
              - resourcedetection/aks
              - resource/hostname
            exporters:
              - debug
              - otlp/gateway
  #  Daemon is a K8s daemonset EDOT collector focused on gathering telemetry at
  #  node level and exposing an OTLP endpoint for data ingestion.
  #  Auto-instrumentation SDKs will use this endpoint.
  daemon:
    fullnameOverride: "opentelemetry-kube-stack-daemon"
    env:
      # Work around for open /mounts error: https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/35990
      - name: HOST_PROC_MOUNTINFO
        value: ""
      - name: ELASTIC_AGENT_OTEL
        value: '"true"'
    volumeMounts:
      - name: varlogpods
        mountPath: /var/log/pods
        readOnly: true
    volumes:
      - name: varlogpods
        hostPath:
          path: /var/log/pods
    presets:
      logsCollection:
        enabled: false # Enable/disable the collection of node's logs.
        storeCheckpoints: true # Store checkpoints for log collection, allowing for resumption from the last processed log.
    hostNetwork: true # Use the host's network namespace. This allows the daemon to access the network interfaces of the host directly.
    securityContext: # Run the daemon as the root user and group for proper metrics collection.
      runAsUser: 0
      runAsGroup: 0
    scrape_configs_file: "" # [Prometheus metrics](https://github.com/open-telemetry/opentelemetry-helm-charts/tree/main/charts/opentelemetry-kube-stack#scrape_configs_file-details)
    config:
      extensions:
        k8s_observer:
          observe_nodes: true
          observe_services: true
          observe_ingresses: true
      exporters:
        # [Debug exporter](https://github.com/open-telemetry/opentelemetry-collector/blob/main/exporter/debugexporter/README.md)
        debug:
          verbosity: basic
        otlp/gateway:
          endpoint: "http://opentelemetry-kube-stack-gateway-collector-headless:4317"
          tls:
            insecure: true
      processors:
        # [Batch Processor](https://github.com/open-telemetry/opentelemetry-collector/tree/main/processor/batchprocessor)
        batch: {} # inherit any values from helm chart
        batch/metrics:
          # explicitly set send_batch_max_size to 0, as splitting metrics requests may cause version_conflict_engine_exception in TSDB
          send_batch_max_size: 0
          timeout: 1s
        # [Resource Detection Processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourcedetectionprocessor)
        resourcedetection/eks:
          detectors: [env, eks] # Detects resources from environment variables and EKS (Elastic Kubernetes Service).
          timeout: 15s
          override: true
          eks:
            resource_attributes:
              k8s.cluster.name:
                enabled: true
        resourcedetection/gcp:
          detectors: [env, gcp] # Detects resources from environment variables and GCP (Google Cloud Platform).
          timeout: 2s
          override: true
        resourcedetection/aks:
          detectors: [env, aks] # Detects resources from environment variables and AKS (Azure Kubernetes Service).
          timeout: 2s
          override: true
          aks:
            resource_attributes:
              k8s.cluster.name:
                enabled: true
        resource/hostname:
          attributes:
            - key: host.name
              from_attribute: k8s.node.name
              action: upsert
        resourcedetection/system:
          detectors: ["system", "ec2"] # Detects resources from the system and EC2 instances.
          system:
            hostname_sources: ["os"]
            resource_attributes:
              host.name:
                enabled: true
              host.id:
                enabled: false
              host.arch:
                enabled: true
              host.ip:
                enabled: true
              host.mac:
                enabled: true
              host.cpu.vendor.id:
                enabled: true
              host.cpu.family:
                enabled: true
              host.cpu.model.id:
                enabled: true
              host.cpu.model.name:
                enabled: true
              host.cpu.stepping:
                enabled: true
              host.cpu.cache.l2.size:
                enabled: true
              os.description:
                enabled: true
              os.type:
                enabled: true
          ec2:
            resource_attributes:
              host.name:
                enabled: false
              host.id:
                enabled: true
        # [Resource Processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourceprocessor)
        resource/k8s: # Resource attributes tailored for services within Kubernetes.
          attributes:
            - key: service.name # Set the service.name resource attribute based on the well-known app.kubernetes.io/name label
              from_attribute: app.label.name
              action: insert
            - key: service.name # Set the service.name resource attribute based on the k8s.container.name attribute
              from_attribute: k8s.container.name
              action: insert
            - key: app.label.name # Delete app.label.name attribute previously used for service.name
              action: delete
            - key: service.version # Set the service.version resource attribute based on the well-known app.kubernetes.io/version label
              from_attribute: app.label.version
              action: insert
            - key: app.label.version # Delete app.label.version attribute previously used for service.version
              action: delete
        resource/cloud:
          attributes:
            - key: cloud.instance.id
              from_attribute: host.id
              action: insert
        # [K8s Attributes Processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/k8sattributesprocessor)
        k8sattributes:
          filter:
            # Only retrieve pods running on the same node as the collector
            node_from_env_var: OTEL_K8S_NODE_NAME
          passthrough: false
          pod_association:
            # Below association takes a look at the k8s.pod.ip and k8s.pod.uid resource attributes or connection's context, and tries to match it with the pod having the same attribute.
            - sources:
                - from: resource_attribute
                  name: k8s.pod.ip
            - sources:
                - from: resource_attribute
                  name: k8s.pod.uid
            - sources:
                - from: connection
          extract:
            metadata:
              - "k8s.namespace.name"
              - "k8s.deployment.name"
              - "k8s.replicaset.name"
              - "k8s.statefulset.name"
              - "k8s.daemonset.name"
              - "k8s.cronjob.name"
              - "k8s.job.name"
              - "k8s.node.name"
              - "k8s.pod.name"
              - "k8s.pod.ip"
              - "k8s.pod.uid"
              - "k8s.pod.start_time"
            labels:
              - tag_name: app.label.name
                key: app.kubernetes.io/name
                from: pod
              - tag_name: app.label.version
                key: app.kubernetes.io/version
                from: pod
        k8sattributes/logs:
          filter:
            # Only retrieve pods running on the same node as the collector
            node_from_env_var: OTEL_K8S_NODE_NAME
          passthrough: false
          pod_association:
            # Below association takes a look at the k8s.pod.ip and k8s.pod.uid resource attributes or connection's context, and tries to match it with the pod having the same attribute.
            - sources:
                - from: resource_attribute
                  name: k8s.pod.ip
            - sources:
                - from: resource_attribute
                  name: k8s.pod.uid
            - sources:
                - from: connection
          extract:
            annotations:
              - tag_name: com.example.logs.format
                key: com.example.logs/format
                from: pod
              - tag_name: com.example.logs.flask
                key: com.example.logs/flask
                from: pod
        transform/parse_flask:
          error_mode: ignore
          log_statements:
            - context: log
              conditions:
                - body != nil and resource.attributes["com.example.logs.flask"] == "true"
              statements:
                - set(cache, ExtractGrokPatterns(body, "%{IP:client_address} - - \\[%{GREEDYDATA:time}\\] \x22.*(?<method>(GET|POST|PUT|DELETE|HEAD|OPTIONS|PATCH|CONNECT|TRACE)) %{URIPATH:path}(?:%{URIPARAM:param})? %{WORD:protocol_name}/%{NUMBER:protocol_version}.*\x22 %{NUMBER:status_code} -", true))
                - set(attributes["http.method"], cache["method"]) where cache["method"] != nil
                - set(attributes["http.url"], Concat([cache["path"], cache["query"]], "")) where cache["path"] != nil and cache["query"] != nil
                - set(attributes["url.path"], cache["path"]) where cache["path"] != nil
                - set(attributes["url.query"], cache["query"]) where cache["query"] != nil            
                - set(attributes["http.status_code"], cache["status_code"]) where cache["status_code"] != nil       
                - set(attributes["client.address"], cache["client_address"]) where cache["client_address"] != nil   
                - set(attributes["network.protocol.name"], cache["protocol_name"]) where cache["protocol_name"] != nil 
                - set(attributes["network.protocol.version"], cache["protocol_version"]) where cache["protocol_version"] != nil 
                - set(time, Time(Concat([cache["time"], "UTC"], " "), "%d/%b/%Y %H:%M:%S %Z")) where cache["time"] != nil 
      receivers:
        # [OTLP Receiver](https://github.com/open-telemetry/opentelemetry-collector/tree/main/receiver/otlpreceiver)
        otlp:
          protocols:
            grpc:
              endpoint: 0.0.0.0:4317
            http:
              endpoint: 0.0.0.0:4318
        receiver_creator/logs:
          watch_observers: [k8s_observer]
          discovery:
            enabled: true
          default_annotations:
            io.opentelemetry.discovery.logs/enabled: "true"
          receivers: {}
        # [Hostmetrics Receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/hostmetricsreceiver)
        hostmetrics:
          collection_interval: 10s
          root_path: /hostfs # Mounted node's root file system
          scrapers:
            cpu:
              metrics:
                system.cpu.utilization:
                  enabled: true
                system.cpu.logical.count:
                  enabled: true
            memory:
              metrics:
                system.memory.utilization:
                  enabled: true
            # process scraper is disabled for now: https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/39423
            #process:
            #  mute_process_exe_error: true
            #  mute_process_io_error: true
            #  mute_process_user_error: true
            #  metrics:
            #    process.threads:
            #      enabled: true
            #    process.open_file_descriptors:
            #      enabled: true
            #    process.memory.utilization:
            #      enabled: true
            #    process.disk.operations:
            #      enabled: true
            network: {}
            processes: {}
            load: {}
            disk: {}
            filesystem:
              exclude_mount_points:
                mount_points:
                  - /dev/*
                  - /proc/*
                  - /sys/*
                  - /run/k3s/containerd/*
                  - /var/lib/docker/*
                  - /var/lib/kubelet/*
                  - /snap/*
                match_type: regexp
              exclude_fs_types:
                fs_types:
                  - autofs
                  - binfmt_misc
                  - bpf
                  - cgroup2
                  - configfs
                  - debugfs
                  - devpts
                  - devtmpfs
                  - fusectl
                  - hugetlbfs
                  - iso9660
                  - mqueue
                  - nsfs
                  - overlay
                  - proc
                  - procfs
                  - pstore
                  - rpc_pipefs
                  - securityfs
                  - selinuxfs
                  - squashfs
                  - sysfs
                  - tracefs
                match_type: strict
        # [Kubelet Stats Receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/kubeletstatsreceiver)
        kubeletstats:
          auth_type: serviceAccount # Authentication mechanism with the Kubelet endpoint, refer to: https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/kubeletstatsreceiver#configuration
          collection_interval: 20s
          endpoint: ${env:OTEL_K8S_NODE_NAME}:10250
          node: '${env:OTEL_K8S_NODE_NAME}'
          # Required to work for all CSPs without an issue
          insecure_skip_verify: true
          k8s_api_config:
            auth_type: serviceAccount
          metrics:
            k8s.pod.memory.node.utilization:
              enabled: true
            k8s.pod.cpu.node.utilization:
              enabled: true
            k8s.container.cpu_limit_utilization:
              enabled: true
            k8s.pod.cpu_limit_utilization:
              enabled: true
            k8s.container.cpu_request_utilization:
              enabled: true
            k8s.container.memory_limit_utilization:
              enabled: true
            k8s.pod.memory_limit_utilization:
              enabled: true
            k8s.container.memory_request_utilization:
              enabled: true
            k8s.node.uptime:
              enabled: true
            k8s.node.cpu.usage:
              enabled: true
            k8s.pod.cpu.usage:
              enabled: true
          extra_metadata_labels:
            - container.id

      # [Service Section](https://opentelemetry.io/docs/collector/configuration/#service)
      connectors:
        otlpjson: {}
        routing/logs:
          default_pipelines: [logs/node]
          table:
            - context: resource
              condition: attributes["com.example.logs.format"] == "otlpjson"
              pipelines: [logs/node_raw_otlpjson]
      service:
        extensions: [k8s_observer]
        pipelines:
          logs/node_raw:
            receivers: [receiver_creator/logs]
            processors: [k8sattributes/logs]
            exporters: [routing/logs]
          logs/node_raw_otlpjson:
            receivers: [routing/logs]
            exporters: [otlpjson]
          logs/node:
            receivers:
              - routing/logs
            processors:
              - batch
              - k8sattributes
              - resourcedetection/system
              - resourcedetection/eks
              - resourcedetection/gcp
              - resourcedetection/aks
              - resource/k8s
              - resource/hostname
              - resource/cloud
            exporters:
              - otlp/gateway
          metrics/node/otel:
            receivers:
              - kubeletstats
              - hostmetrics
            processors:
              - batch/metrics
              - k8sattributes
              - resourcedetection/system
              - resourcedetection/eks
              - resourcedetection/gcp
              - resourcedetection/aks
              - resource/k8s
              - resource/hostname
              - resource/cloud
            exporters:
              # - debug
              - otlp/gateway
          metrics/otel-apm:
            receivers:
              - otlp
            processors:
              - batch/metrics
              - resource/hostname
            exporters:
              - otlp/gateway
          logs/apm:
            receivers:
              - otlp
              - otlpjson
            processors:
              - k8sattributes/logs
              - transform/parse_flask
              - batch
              - resource/hostname
            exporters:
              - otlp/gateway
          traces/apm:
            receivers:
              - otlp
            processors:
              - batch
              - resource/hostname
            exporters:
              - otlp/gateway
  #  Gateway is a K8s deployment EDOT collector focused on processing and
  #  forwarding telemetry to an Elasticsearch endpoint.
  gateway:
    fullnameOverride: "opentelemetry-kube-stack-gateway"
    resources:
      limits:
        cpu: 1500m
        memory: 1500Mi
      requests:
        cpu: 100m
        memory: 500Mi
    suffix: gateway
    replicas: 2
    enabled: true
    env:
      - name: ELASTIC_AGENT_OTEL
        value: '"true"'
      - name: ELASTIC_ENDPOINT
        valueFrom:
          secretKeyRef:
            name: elastic-secret-otel
            key: elastic_endpoint
      - name: ELASTIC_API_KEY
        valueFrom:
          secretKeyRef:
            name: elastic-secret-otel
            key: elastic_api_key
      - name: GOMAXPROCS
        valueFrom:
          resourceFieldRef:
            resource: limits.cpu
      - name: GOMEMLIMIT
        value: "1025MiB"
    config:
      connectors:
        routing:
          default_pipelines: [metrics/otel]
          error_mode: ignore
          table:
            - context: metric
              statement: route() where instrumentation_scope.name == "github.com/open-telemetry/opentelemetry-collector-contrib/receiver/kubeletstatsreceiver" or IsMatch(instrumentation_scope.name, "github.com/open-telemetry/opentelemetry-collector-contrib/receiver/hostmetricsreceiver/internal/scraper/*")
              pipelines: [metrics/infra/ecs, metrics/otel]
        # [Elastic APM Connector](https://github.com/elastic/opentelemetry-collector-components/tree/main/connector/elasticapmconnector)
        elasticapm: {}
      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: ${env:MY_POD_IP}:4317
            http:
              endpoint: ${env:MY_POD_IP}:4318
      processors:
        # [Elastic Infra Metrics Processor](https://github.com/elastic/opentelemetry-collector-components/tree/main/processor/elasticinframetricsprocessor)
        elasticinframetrics:
          add_system_metrics: true
          add_k8s_metrics: true
          drop_original: true
        # [Attributes Processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/attributesprocessor)
        attributes/dataset:
          actions:
            - key: event.dataset
              from_attribute: data_stream.dataset
              action: upsert
        resource/process:
          attributes:
            - key: process.executable.name
              action: delete
            - key: process.executable.path
              action: delete
        batch:
          send_batch_size: 1000
          timeout: 1s
          send_batch_max_size: 1500
        batch/metrics:
          # explicitly set send_batch_max_size to 0, as splitting metrics requests may cause version_conflict_engine_exception in TSDB
          send_batch_max_size: 0
          timeout: 1s
        # [Elastic Trace Processor](https://github.com/elastic/opentelemetry-collector-components/tree/main/processor/elastictraceprocessor)
        elastictrace: {} # The processor enriches traces with elastic specific requirements.
      exporters:
        debug: {}
        # [Elasticsearch exporter](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/elasticsearchexporter/README.md)
        elasticsearch/otel:
          endpoints: # List of Elasticsearch endpoints.
            - ${env:ELASTIC_ENDPOINT}
          api_key: ${env:ELASTIC_API_KEY} # API key for Elasticsearch authentication.
          logs_dynamic_index:
            enabled: true
          metrics_dynamic_index:
            enabled: true
          traces_dynamic_index:
            enabled: true
          # Enable in order to skip the SSL certificate Check
          # tls:
          #   insecure_skip_verify: true
          mapping:
            mode: otel
        elasticsearch/ecs:
          endpoints:
            - ${env:ELASTIC_ENDPOINT}
          api_key: ${env:ELASTIC_API_KEY}
          mapping:
            mode: ecs
      service:
        pipelines:
          metrics:
            receivers: [otlp]
            exporters: [routing]
          metrics/infra/ecs:
            receivers: [routing]
            processors:
              - elasticinframetrics
              - attributes/dataset
              - resource/process
              - batch/metrics
            exporters: [debug, elasticsearch/ecs]
          metrics/otel:
            receivers: [routing]
            processors: [batch]
            exporters: [debug, elasticsearch/otel]
          logs:
            receivers: [otlp]
            processors: [batch]
            exporters: [debug, elasticapm, elasticsearch/otel]
          traces:
            receivers: [otlp]
            processors: [batch, elastictrace]
            exporters: [debug, elasticapm, elasticsearch/otel]
          metrics/aggregated-otel-metrics:
            receivers:
              - elasticapm
            processors:
            exporters:
              - debug
              - elasticsearch/otel
# For more details on OpenTelemetry's zero-code instrumentation, see:
# https://opentelemetry.io/docs/concepts/instrumentation/zero-code/
instrumentation:
  name: elastic-instrumentation
  enabled: true # Enable/disable auto-instrumentation.
  exporter:
    endpoint: http://opentelemetry-kube-stack-daemon-collector.opentelemetry-operator-system.svc.cluster.local:4318 # The daemonset OpenTelemetry Collector endpoint where telemetry data will be exported.
  propagators:
    - tracecontext # W3C TraceContext propagator for distributed tracing.
    - baggage # Baggage propagator to include baggage information in trace context.
    - b3 # B3 propagator for Zipkin-based distributed tracing compatibility.
  sampler:
    type: parentbased_traceidratio # Sampler type
    argument: "1.0" # Sampling rate set to 100% (all traces are sampled).
  java:
    image: docker.elastic.co/observability/elastic-otel-javaagent:1.3.0
  nodejs:
    image: docker.elastic.co/observability/elastic-otel-node:1.0.0
  dotnet:
    image: docker.elastic.co/observability/elastic-otel-dotnet:1.0.1
  python:
    image: docker.elastic.co/observability/elastic-otel-python:1.0.0
  go:
    image: ghcr.io/open-telemetry/opentelemetry-go-instrumentation/autoinstrumentation-go:v0.21.0